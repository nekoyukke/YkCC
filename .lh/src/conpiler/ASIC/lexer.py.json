{
    "sourceFile": "src/conpiler/ASIC/lexer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1760605122779,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1760605129297,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,9 +24,9 @@\n \r\n class Lexer:\r\n     KEYWORDS = {\r\n         \"LET\",\"IF\",\"THEN\",\"GOTO\",\"FOR\",\"NEXT\",\"PRINT\",\"INPUT\",\"DIM\",\"END\",\r\n-        \"GOSUB\",\"RETURN\",\"STEP\",\"TO\",\"ELSE\",\"REM\",\"AND\",\"OR\",\"NOT\", \"\"\r\n+        \"GOSUB\",\"RETURN\",\"STEP\",\"TO\",\"ELSE\",\"REM\",\"AND\",\"OR\",\"NOT\",\"FUNC\"\r\n     }\r\n \r\n     # Ordered list of token specifications (longer/priority patterns first)\r\n     token_specification = [\r\n"
                }
            ],
            "date": 1760605122779,
            "name": "Commit-0",
            "content": "# BASIC lexer implementation (Python)\r\n# This lexer is simple but practical for a BASIC-like language.\r\n# Token types: LINE_NUM, NUMBER, IDENT, KEYWORD, STRING, OP, COMMA, COLON, NEWLINE, EOF\r\n# Supports comments starting with REM (case-insensitive) or single-quote ('), and ignores whitespace.\r\n# Keywords: LET, IF, THEN, GOTO, FOR, NEXT, PRINT, INPUT, DIM, END, GOSUB, RETURN, STEP, TO, ELSE, REM\r\n# Example usage: run the file or call Lexer().tokenize(basic_source)\r\n\r\nimport re\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Tuple, Optional\r\n\r\n@dataclass\r\nclass Token:\r\n    type: str\r\n    value: str\r\n    lineno: int\r\n    col: int\r\n\r\n    def __repr__(self):\r\n        return f\"Token({self.type!r}, {self.value!r}, line={self.lineno}, col={self.col})\"\r\n\r\nclass LexerError(Exception):\r\n    pass\r\n\r\nclass Lexer:\r\n    KEYWORDS = {\r\n        \"LET\",\"IF\",\"THEN\",\"GOTO\",\"FOR\",\"NEXT\",\"PRINT\",\"INPUT\",\"DIM\",\"END\",\r\n        \"GOSUB\",\"RETURN\",\"STEP\",\"TO\",\"ELSE\",\"REM\",\"AND\",\"OR\",\"NOT\", \"\"\r\n    }\r\n\r\n    # Ordered list of token specifications (longer/priority patterns first)\r\n    token_specification = [\r\n        (\"LINE_NUM\", r'^[ \\t]*\\d+'),               # Line number at start of a line\r\n        (\"STRING\",   r'\"([^\"]*)\"'),                # \"string literal\"\r\n        (\"NUMBER\",   r'\\d+(\\.\\d+)?'),              # Integer or decimal number\r\n        (\"IDENT\",    r'[A-Za-z_][A-Za-z0-9_]*'),   # Identifiers and keywords\r\n        (\"CMPOP\",    r'<=|>=|!=|==|<|>'),       # Operators\r\n        (\"OP\",       r'=|\\+|-|\\*|/|%'),            # Operators\r\n        (\"COMMA\",    r','),                        # Comma\r\n        (\"COLON\",    r':'),                        # Colon (also used for label separator)\r\n        (\"LPAREN\",   r'\\('),\r\n        (\"RPAREN\",   r'\\)'),\r\n        (\"WS\",       r'[ \\t]+'),                   # Whitespace (skipped)\r\n        (\"NEWLINE\",  r'\\n'),                       # Line endings\r\n        (\"MISMATCH\", r'.'),                        # Any other single character\r\n    ]\r\n\r\n    def __init__(self):\r\n        parts = []\r\n        for name, pattern in self.token_specification:\r\n            parts.append(f\"(?P<{name}>{pattern})\")\r\n        self.master_re = re.compile(\"|\".join(parts), re.IGNORECASE | re.MULTILINE)\r\n\r\n        # regex for REM-style comment: either starting with REM or '\r\n        self.rem_re = re.compile(r\"^\\s*(?:REM\\b|')\", re.IGNORECASE)\r\n\r\n    def tokenize(self, code: str) -> List[Token]:\r\n        tokens: List[Token] = []\r\n        lines = code.splitlines(keepends=True)\r\n        lineno = 1\r\n        for line in lines:\r\n            pos = 0\r\n            line_len = len(line)\r\n\r\n            # handle REM/comments at start or later: find first REM or ' that is not in a string\r\n            # We'll handle comments by truncating the line at the first occurrence of \" REM \" or starting \"'\"\r\n            # But be careful not to cut inside a string literal.\r\n            truncated = self._strip_comment(line)\r\n            if truncated is None:\r\n                # entire line is comment; emit NEWLINE token only\r\n                tokens.append(Token(\"NEWLINE\", \"\\\\n\", lineno, 1))\r\n                lineno += 1\r\n                continue\r\n\r\n            line_to_scan = truncated\r\n            # If the line begins with a line number, we want to capture it as a single token (LINE_NUM)\r\n            m = re.match(r'^[ \\t]*(\\d+)', line_to_scan)\r\n            if m:\r\n                val = m.group(1)\r\n                col = m.start(1) + 1\r\n                tokens.append(Token(\"LINE_NUM\", val, lineno, col))\r\n                pos = m.end(1)\r\n            # scan the rest of the line\r\n            for mo in self.master_re.finditer(line_to_scan, pos):\r\n                kind = mo.lastgroup\r\n                value = mo.group(kind)\r\n                col = mo.start() + 1\r\n                if kind == \"WS\":\r\n                    continue\r\n                if kind == \"NEWLINE\":\r\n                    tokens.append(Token(\"NEWLINE\", \"\\\\n\", lineno, col))\r\n                elif kind == \"STRING\":\r\n                    # strip surrounding quotes\r\n                    inner = mo.group(kind)[1:-1]\r\n                    tokens.append(Token(\"STRING\", inner, lineno, col))\r\n                elif kind == \"IDENT\":\r\n                    up = value.upper()\r\n                    if up in self.KEYWORDS:\r\n                        tokens.append(Token(\"KEYWORD\", up, lineno, col))\r\n                    else:\r\n                        tokens.append(Token(\"IDENT\", value, lineno, col))\r\n                elif kind == \"NUMBER\":\r\n                    tokens.append(Token(\"NUMBER\", value, lineno, col))\r\n                elif kind == \"MISMATCH\":\r\n                    raise LexerError(f\"Unexpected character {value!r} at line {lineno} col {col}\")\r\n                else:\r\n                    tokens.append(Token(kind, value, lineno, col))\r\n                pos = mo.end()\r\n            # if line did not end with NEWLINE token, append one (we keep line-level semantics)\r\n            if not (tokens and tokens[-1].type == \"NEWLINE\"):\r\n                tokens.append(Token(\"NEWLINE\", \"\\\\n\", lineno, line_len))\r\n            lineno += 1\r\n\r\n        tokens.append(Token(\"EOF\", \"\", lineno, 1))\r\n        return tokens\r\n\r\n    def _strip_comment(self, line: str) -> Optional[str]:\r\n        \"\"\"\r\n        Returns the line truncated before an unquoted REM or single-quote comment marker.\r\n        If the whole line is a comment, returns None.\r\n        \"\"\"\r\n        i = 0\r\n        in_string = False\r\n        while i < len(line):\r\n            ch = line[i]\r\n            if ch == '\"':\r\n                in_string = not in_string\r\n                i += 1\r\n                continue\r\n            if not in_string:\r\n                # check for single-quote comment\r\n                if ch == \"'\":\r\n                    # if it's at beginning or after whitespace, treat as comment start\r\n                    # truncate from here\r\n                    prefix = line[:i]\r\n                    if prefix.strip() == \"\":\r\n                        return None if i == 0 else prefix\r\n                    return prefix\r\n                # check for REM keyword (case-insensitive), ensure it's a separate word\r\n                rem_match = re.match(r'REM\\b', line[i:], re.IGNORECASE)\r\n                if rem_match:\r\n                    # ensure REM is either at column 0 / after whitespace or after separators\r\n                    prefix = line[:i]\r\n                    if prefix.strip() == \"\":\r\n                        # entire rest is comment\r\n                        return None if i == 0 else prefix\r\n                    return prefix\r\n            i += 1\r\n        return line\r\n\r\n# --- Demo ---\r\nif __name__ == \"__main__\":\r\n    src = '''10 LET A = 5\r\n20 LET B = A + 2\r\n30 IF B > 10 THEN GOTO 10\r\n40 PRINT \"HELLO, WORLD\"  ' this is a comment\r\n50 REM full line comment\r\n60 FOR I = 0 TO 3\r\n70 NEXT I\r\n80 END\r\n'''\r\n    lexer = Lexer()\r\n    toks = lexer.tokenize(src)\r\n    for t in toks:\r\n        print(t)\r\n"
        }
    ]
}